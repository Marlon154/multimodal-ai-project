@article{bayoudh2022survey,
  title = {A Survey on Deep Multimodal Learning for Computer Vision: Advances, Trends, Applications, and Datasets},
  shorttitle = {A Survey on Deep Multimodal Learning for Computer Vision},
  author = {Bayoudh, Khaled and Knani, Raja and Hamdaoui, Fayçal and Mtibaa, Abdellatif},
  date = {2022-08-01},
  journaltitle = {The Visual Computer},
  shortjournal = {Vis Comput},
  volume = {38},
  number = {8},
  pages = {2939--2970},
  issn = {1432-2315},
  doi = {10.1007/s00371-021-02166-7},
  url = {https://doi.org/10.1007/s00371-021-02166-7},
  urldate = {2024-05-27},
  abstract = {The research progress in multimodal learning has grown rapidly over the last decade in several areas, especially in computer vision. The growing potential of multimodal data streams and deep learning algorithms has contributed to the increasing universality of deep multimodal learning. This involves the development of models capable of processing and analyzing the multimodal information uniformly. Unstructured real-world data can inherently take many forms, also known as modalities, often including visual and textual content. Extracting relevant patterns from this kind of data is still a motivating goal for researchers in deep learning. In this paper, we seek to improve the understanding of key concepts and algorithms of deep multimodal learning for the computer vision community by exploring how to generate deep models that consider the integration and combination of heterogeneous visual cues across sensory modalities. In particular, we summarize six perspectives from the current literature on deep multimodal learning, namely: multimodal data representation, multimodal fusion (i.e., both traditional and deep learning-based schemes), multitask learning, multimodal alignment, multimodal transfer learning, and zero-shot learning. We also survey current multimodal applications and present a collection of benchmark datasets for solving problems in various vision domains. Finally, we highlight the limitations and challenges of deep multimodal learning and provide insights and directions for future research.},
  langid = {english},
  keywords = {Applications,Computer vision,Datasets,Deep learning,Multimodal learning,Sensory modalities},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/63DEDY9R/Bayoudh et al. - 2022 - A survey on deep multimodal learning for computer .pdf}
}

@online{biten2019good,
  title = {Good {{News}}, {{Everyone}}! {{Context}} Driven Entity-Aware Captioning for News Images},
  author = {Biten, Ali Furkan and Gomez, Lluis and Rusiñol, Marçal and Karatzas, Dimosthenis},
  date = {2019-04-02},
  eprint = {1904.01475},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.01475},
  urldate = {2024-05-06},
  abstract = {Current image captioning systems perform at a merely descriptive level, essentially enumerating the objects in the scene and their relations. Humans, on the contrary, interpret images by integrating several sources of prior knowledge of the world. In this work, we aim to take a step closer to producing captions that offer a plausible interpretation of the scene, by integrating such contextual information into the captioning pipeline. For this we focus on the captioning of images used to illustrate news articles. We propose a novel captioning method that is able to leverage contextual information provided by the text of news articles associated with an image. Our model is able to selectively draw information from the article guided by visual cues, and to dynamically extend the output dictionary to out-of-vocabulary named entities that appear in the context source. Furthermore we introduce `GoodNews', the largest news image captioning dataset in the literature and demonstrate state-of-the-art results.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/RYUIJBY6/Biten et al. - 2019 - Good News, Everyone! Context driven entity-aware c.pdf;/home/marlon/snap/zotero-snap/common/Zotero/storage/8H86UB3A/1904.html}
}

@online{dosovitskiy2021image,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2024-06-02},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {preprint},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/DL7GN5ZM/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf}
}

@inproceedings{hu2023opendomain,
  title = {Open-Domain {{Visual Entity Recognition}}: {{Towards Recognizing Millions}} of {{Wikipedia Entities}}},
  shorttitle = {Open-Domain {{Visual Entity Recognition}}},
  author = {Hu, Hexiang and Luan, Yi and Chen, Yang and Khandelwal, Urvashi and Joshi, Mandar and Lee, Kenton and Toutanova, Kristina and Chang, Ming-Wei},
  date = {2023},
  pages = {12065--12075},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.html},
  urldate = {2024-05-28},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/ESEVTSZB/Hu et al. - 2023 - Open-domain Visual Entity Recognition Towards Rec.pdf}
}

@online{kirillov2023segment,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
  date = {2023-04-05},
  eprint = {2304.02643},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.02643},
  url = {http://arxiv.org/abs/2304.02643},
  urldate = {2024-06-08},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/ZWSECBMQ/Kirillov et al. - 2023 - Segment Anything.pdf;/home/marlon/snap/zotero-snap/common/Zotero/storage/LH2PBG3B/2304.html}
}

@online{lai2024lisa,
  title = {{{LISA}}: {{Reasoning Segmentation}} via {{Large Language Model}}},
  shorttitle = {{{LISA}}},
  author = {Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  date = {2024-05-01},
  eprint = {2308.00692},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.00692},
  url = {http://arxiv.org/abs/2308.00692},
  urldate = {2024-06-08},
  abstract = {Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a {$<$}SEG{$>$} token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/ASHJ6KQ7/Lai et al. - 2024 - LISA Reasoning Segmentation via Large Language Mo.pdf;/home/marlon/snap/zotero-snap/common/Zotero/storage/M2P6ZYN2/2308.html}
}

@article{liu2022edmf,
  title = {{{EDMF}}: {{Efficient Deep Matrix Factorization With Review Feature Learning}} for {{Industrial Recommender System}}},
  shorttitle = {{{EDMF}}},
  author = {Liu, Hai and Zheng, Chao and Li, Duantengchuan and Shen, Xiaoxuan and Lin, Ke and Wang, Jiazhang and Zhang, Zhen and Zhang, Zhaoli and Xiong, Neal N.},
  date = {2022-07},
  journaltitle = {IEEE Transactions on Industrial Informatics},
  volume = {18},
  number = {7},
  pages = {4361--4371},
  issn = {1941-0050},
  doi = {10.1109/TII.2021.3128240},
  url = {https://ieeexplore.ieee.org/abstract/document/9616457?casa_token=GpoV86__cXIAAAAA:E0V90aah6nYyeOvdCX3v0rjMImYKI9pa9a3Ax_aUtRIodyhIULpoWdpHoyEyXE-n5RtLnzl74YE},
  urldate = {2024-05-27},
  abstract = {Recommendation accuracy is a fundamental problem in the quality of the recommendation system. In this article, we propose an efficient deep matrix factorization (EDMF) with review feature learning for the industrial recommender system. Two characteristics in user’s review are revealed. First, interactivity between the user and the item, which can also be considered as the former’s scoring behavior on the latter, is exploited in a review. Second, the review is only a partial description of the user’s preferences for the item, which is revealed as the sparsity property. Specifically, in the first characteristic, EDMF extracts the interactive features of onefold review by convolutional neural networks with word-attention mechanism. Subsequently, L\_0 norm is leveraged to constrain the review considering that the review information is a sparse feature, which is the second characteristic. Furthermore, the loss function is constructed by maximum a posteriori estimation theory, where the interactivity and sparsity property are converted as two prior probability functions. Finally, the alternative minimization algorithm is introduced to optimize the loss functions. Experimental results on several datasets demonstrate that the proposed methods, which show good industrial conversion application prospects, outperform the state-of-the-art methods in terms of effectiveness and efficiency.},
  eventtitle = {{{IEEE Transactions}} on {{Industrial Informatics}}},
  keywords = {Collaboration,Deep matrix factorization,Feature extraction,Image color analysis,industrial recommender system,Informatics,interactivity,L_0 norm,Predictive models,Recommender systems,sparsity property,Training},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/8CG4F3MC/9616457.html}
}

@article{stefanini2023show,
  title = {From {{Show}} to {{Tell}}: {{A Survey}} on {{Deep Learning-Based Image Captioning}}},
  shorttitle = {From {{Show}} to {{Tell}}},
  author = {Stefanini, Matteo and Cornia, Marcella and Baraldi, Lorenzo and Cascianelli, Silvia and Fiameni, Giuseppe and Cucchiara, Rita},
  date = {2023-01},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  shortjournal = {IEEE Trans Pattern Anal Mach Intell},
  volume = {45},
  number = {1},
  eprint = {35130142},
  eprinttype = {pmid},
  pages = {539--559},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3148210},
  abstract = {Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.},
  langid = {english},
  keywords = {Algorithms,Benchmarking,Deep Learning,Language,Natural Language Processing},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/2B6PEPTJ/Stefanini et al. - 2023 - From Show to Tell A Survey on Deep Learning-Based.pdf}
}

@online{tran2020transform,
  title = {Transform and {{Tell}}: {{Entity-Aware News Image Captioning}}},
  shorttitle = {Transform and {{Tell}}},
  author = {Tran, Alasdair and Mathews, Alexander and Xie, Lexing},
  date = {2020-06-12},
  eprint = {2004.08070},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.08070},
  urldate = {2024-06-02},
  abstract = {We propose an end-to-end model which generates captions for images embedded in news articles. News images present two key challenges: they rely on real-world knowledge, especially about named entities; and they typically have linguistically rich captions that include uncommon words. We address the first challenge by associating words in the caption with faces and objects in the image, via a multi-modal, multi-head attention mechanism. We tackle the second challenge with a state-of-the-art transformer language model that uses byte-pair-encoding to generate captions as a sequence of word parts. On the GoodNews dataset, our model outperforms the previous state of the art by a factor of four in CIDEr score (13 to 54). This performance gain comes from a unique combination of language models, word representation, image embeddings, face embeddings, object embeddings, and improvements in neural network design. We also introduce the NYTimes800k dataset which is 70\% larger than GoodNews, has higher article quality, and includes the locations of images within articles as an additional contextual cue.},
  pubstate = {preprint},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/YB8UWPNB/Tran et al. - 2020 - Transform and Tell Entity-Aware News Image Captioning.pdf}
}

@article{zheng2020spatial,
  title = {Spatial Attention Based Visual Semantic Learning for Action Recognition in Still Images},
  author = {Zheng, Yunpeng and Zheng, Xiangtao and Lu, Xiaoqiang and Wu, Siyuan},
  date = {2020-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {413},
  pages = {383--396},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.07.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220311231},
  urldate = {2024-06-05},
  langid = {english},
  file = {/home/marlon/snap/zotero-snap/common/Zotero/storage/4EC75FXV/Zheng et al. - 2020 - Spatial attention based visual semantic learning f.pdf}
}
